---
title: "Lecture 4 Multiple Regression"
author: "Nischal Pokhrel"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multiple Regression

We're back to simulating the predictor variables directly, and then taking the response from those - this is a more typical way to simulate!

QUESTION: What parameters are you imagining in your simulations?
Answer-I want to see vraT expression by *S. aureus* mutants under different environmental conditions like salt concentration, heavy metals, acidic ph and temperature

```{r Simulate Multiple parameters which affect the response variable}
library(MASS) ### this might be a package you need to install

##simulate multivariate normal data - this means that our parameters are related to each other
sigma <- matrix(c(1, 0.25, 0.3, 0.2,
                   0.25, 1, 0.4, 0.15,
                   0.3, 0.4, 1, 0.25,
                   0.2, 0.15, 0.25, 1), nrow = 4) ## positive-definite symmetric matrix specifying the covariance matrix of the variables - what does this mean? Make you sure can explain to yourself what this line does.This is a covariance matrix that shows covariance among variables. THis is a 4X4 covariance matrix meaning it has 4 variable with 1st row and column for 1st variable, 2nd row and 2nd column for 2nd variable and so on. 

### This line gives us 3 parameters - how many do we want?Answer-I want 4 so I am making some adjustments
predictors <- mvrnorm(1000, rep(50, 4), sigma) ###we are simulating the predictors here. What have we told this command to do? How correlated are the predictors?
cor.test(predictors[, 1], predictors[, 2])

```

QUESTION: How does changing sigma on line 22 change the correlations on line 28?
Answer-The sigma represents the covariance matrix which now has 4 predictor variable in place of 3. Also the covariance has been increased so the correlation calculated should be higher. 


QUESTION: What response variable are you imagining? How is it distributed? 
Answer-Expression level of vraT which should be normally distributed.
QUESTION: What relationship are you simulating between your predictor variables and your response variable? 
Answer-A change in expression level depending on the environmental condition
QUESTION: How much error are you simulating? How much irreduciable error do you expect in your response variable of interest? 
Answer- I have stimulated some random errors in the simulation and I am expecting some random mutations.

```{r Simulate a response variable which all three variables affects the response variable}
response <- 1.5 * predictors[, 1] + 0.75 * predictors[, 2] + 
            0.05 * predictors[, 3] + 0.1 * predictors[, 4] +  # Added fourth predictor
            rnorm(1000, 50, 1)   ### change this line based on what your response variable and imagined predictors are. Note that I have simulated a relationship between all predictors and the response, and note how much error I have simulated. 

cbind.data.frame(response, predictors)->data
names(data) <- c("VrAT_expression", "salt_concentration", "heavy_metals", "acidic_pH", "temperature") ### PUT INFORMATIVE NAMES HERE FOR WHAT YOU'RE SIMULATING
summary(data)

```

An assumption of multiple regression is that there isn't multicolinarity. 
We are breaking this assumption. 

QUESTION: Explore what happens to your R^2 when you change the strength of correlation between the parameters.
Answer-When I increase strength of correlation than R^2 increases showing that the variable are more dependent on one another.

```{r Run a multiple regression}

lm(VrAT_expression ~ salt_concentration + heavy_metals + acidic_pH + temperature, data=data) -> model1
summary(model1)
anova(model1)

```


There are a couple of ways to plot multiple regressions. Some folks like 3D plots, so they can see the plan of the regression incorporating two predictor variables. This isn't my favourite, as it only works for two variables. 

Here, I'm going to plot using partial residuals - essentially, we're going to plot the relationship of one variable after we've accounted for the other two.


```{r plot multiple regression using partial residuals}

# Plot residuals of the linear model against salt_concentration
plot(data$salt_concentration, resid(lm(VrAT_expression ~ heavy_metals + acidic_pH + temperature, data=data)),
     xlab = "Salt Concentration", ylab = "Residuals", main = "Residuals vs Salt Concentration")
abline(h = 0, col = "red")  # Add a horizontal line at 0 for reference

# Plot residuals against heavy_metals
plot(data$heavy_metals, resid(lm(VrAT_expression ~ salt_concentration + acidic_pH + temperature, data=data)),
     xlab = "Heavy Metals", ylab = "Residuals", main = "Residuals vs Heavy Metals")
abline(h = 0, col = "red")  # Add a horizontal line at 0 for reference

# Plotting residuals against acidic_pH
plot(data$acidic_pH, resid(lm(VrAT_expression ~ salt_concentration + heavy_metals + temperature, data=data)),
     xlab = "Acidic pH", ylab = "Residuals", main = "Residuals vs Acidic pH")
abline(h = 0, col = "red")  # Add a horizontal line at 0 for reference

```

QUESTION: Report the results of your multiple regression as you would in a results section of a paper. 
The analysis shows that salt concentration, heavy metals, and temperature significantly influence VrAT expression, while acidic pH does not have a significant effect.

```{r Run a multiple regression with interactions}

# Fit a linear model with interactions among predictors
lm(VrAT_expression ~ salt_concentration * heavy_metals * acidic_pH * temperature, data=data) -> model2

# Display the summary of the model
summary(model2)

# Perform ANOVA on the model
anova(model2)

```
QUESTION: Do you have evidence of an interaction? How does an interaction change the p-values or R^2? Why is this the case?
Answer-Since none of the p values are shows no significance maybe the model needs to be simplified.

An interaction is when the relationship between the response variable and a predictor variable changes depending on an other predictor variable.

QUESTION: Think of an interaction term!
While i see a positive correlation between the predictor and response variable and simplified model maybe moe appropriate to elucidate this interactions.
```{r simulate an interaction}
# Generate an additional predictor (e.g., temperature variation)
predictor_4 <- rnorm(1000, 50, 1)  # Simulating normally distributed random values

# Define a new response variable based on VrAT expression, adding conditional logic
response2 <- 1.5 * predictors[, 1] + 
             ifelse(predictors[, 1] > 0.4, 0.5, 0) * predictor_4 + 
             rnorm(1000, 50, 1)  # Adding random noise

# Combine the original data with the new response and predictor
cbind.data.frame(data, response2, predictor_4) -> data2

# Assign informative names reflecting the VrAT data context
names(data2) <- c("VrAT_expression", "salt_concentration", "heavy_metals", "acidic_pH", "temperature", "response2", "additional_predictor")

```

QUESTION: Describe in words what your interaction term is doing.
A positive correlation albiet not a significant one.

Below, I am testing a 4 way interaction. Explain why this might or might not be appropriate for your own simulated data. 

```{r analyze with the interaction}
# Fit a linear model with interactions among the new response variable and predictors
lm(response2 ~ salt_concentration * heavy_metals * acidic_pH * predictor_4, data=data2) -> model3

# Display the summary of the model
summary(model3)

# Perform ANOVA on the model
anova(model3)

```

QUESTION: Write a statistical methods paragraph and a statistical results paragraph. Make sure that everything in your statistical methods is reported in your results!
Answer-
I simulated 1000 observations were generated. Even though the multiple linear regression analysis indicated that the model could explained the variance in vraT expression, none of the predictors or interaction terms were statistically significant which suggests that the interactions do not meaningfully contribute to explaining vraT expression variability.Maybe its better to remove some variable and resimulate the simpler model.
